---

layout: post

title: Gradient

date: 2021-09-23 21:05:23 +0900

categories: ML

---

Gradient에 대해서
---

딥러닝에서 가중치를 학습하는 과정에서 필수적으로 사용하는 Gradient를 알아봅니다.

### Gradient는 무엇인가?

Layer를 통한 인공지능 학습을 진행하는 것은 추후 [포스팅]을 진행하겠습니다.

사전적인 의미로 **Gradient는 "기울기"입니다.** 그러면 무엇의 기울기인지 알아봐야 할 것 같습니다.

딥러닝을 통해 우리가 알고 싶은 것은 실제 값의 분포를 표현하는 **어떠한 규칙**에 가능한 가깝게 예측 할 수 있는 모델입니다.

너무나 실제 학습 데이터에 가깝게 예측해버리면 우리는 Over Fitting이라고 하고 반대로 너무 실제 학습 데이터에 멀리 예측해버리면 우리는 Under Fitting이라고 하죠.

**즉, Fitting을 통해 우리는 실제 분포에 가깝게 표현하고 과하지도 부족하지도 않게 학습을 진행합니다.**

Gradient는 이 Fitting을 하는 과정에서 사용되는 기울기입니다.

$$\hat{Y} = xW + b$$

위의 Hyperthesis를 연산하는 과정에서 우리는 $W$와 $b$를 바꾸어 $\hat{Y}$를 실제 값의 분포인 $Y$에 가깝도록 만들어야 합니다.

우리는 실제 값에서 Hyperthesis를 통해 얻어진 값의 차이를 **Loss**라고 표현하며 **우리가 얼마나 잘못 예측했는지**를 알 수 있습니다.

$$Loss_{MSE} = \sum(Y - (xW+b))^2$$

그러면 $W$와 $b$를 최적화하는 것이 무엇을 말하는 걸까요?
일단 **$W$값이 작아지거나 커지거나** 혹은 **$b$값이 커지거나 작아지거나** 와 같은 변화가 발생해야 합니다. 그래야 $\hat{Y}$의 값이 바뀌고 $Loss$가 줄어드니까요.

그럼 이렇게 말할 수 있습니다.

**"$Loss$가 줄어드는 방향으로 $W$와 $b$가 이동하는구나"**

그러면 의문이 구체화됩니다.

**"$Loss$가 줄어드는 방향은 어떤 방향일까?"**

이 의문을 해결하기 위해 Gradient가 존재합니다. 우리는 다음의 과정을 통해 예측을 한다고 생각해보겠습니다.

$$\hat{Y} = xW + b$$

위의 식이 주어질 때 이러한 질문을 받으면 어떻게 대답할 수 있을까요?
$x$가 1커질 때 **$\hat{Y}$** 은 어떻게 변하나요?
답은 "$W$만큼 커집니다."입니다.

이 사실을 우리는 **편미분**을 통해 알 수 있죠. $\hat{Y}$를 $x$로 편미분을 하면 우리가 알고싶은 **x가 1증가 할 때 \hat{Y}에 미치는 영향**을 알 수 있습니다.

이러한 방식을 **Backpropergation**이라고 합니다.

이걸 그대로 적용해서 Layer에 적용해봅시다.

**FC Layer(1) -> Sigmoid Layer -> FC Layer(2) -> Sigmoid Layer -> FC Layer(3) -> $Loss_{Y-Output}$**

Loss에서 역방향으로 편미분을 통해서 각 파라미터가 얼마나 영향을 가지고 있는지 알아봅니다. $Loss$를 $x_3$으로 편미분하면 $W_3$이 $Loss$에 얼마나 영향을 미치는지 알 수 있겠네요. 마찬가지로 $x_3$을 $w_2$로 편미분하면 $x_3$에 $x_2$가 얼마나 영향을 미치는지 알 수 있습니다. ~~물론 Sigmoid를 통과 한 후의 결과에 대한 값이지만, 설명에서는 활성화함수 연산을 무시하겠습니다.~~

그리고 그 값을 식으로 적으면 다음과 같습니다.

$$\frac{\partial{Loss}}{\partial{x_1}} = \frac{\partial{Loss}}{\partial{x_3}} \frac{\partial{x_3}}{\partial{x_2}} \frac{\partial{x_2}}{\partial{x_1}} $$

여기서 $\frac{\partial{Loss}}{\partial{x_1}}$ 가 의미하는 바가 **$W_1$이 Loss에 미치는 영향** 이라는 점을 알면 연산이 얼마나 간소화 되는지 알 수 있습니다. 이러한 방식을 **Chain Rule**을 통한 Gradient 계산이라고 말합니다.

우리는 이제 **$W_i$** 가 $Loss$에 미치는 영향을 알 수 있는 무기가 생겼습니다. 상세하게 말하면 **$W$가 양 또는 음의 영향을 미치는지 추가적으로 얼마나 크게 영향을 미치는지** 알 수 있습니다.

이제 $Loss$를 줄이고 싶다면 $W$를 음의 방향으로 움직이게 하고 반대라면 양의 방향으로 움직이게 하면 됩니다.

해당 미분값이 양수라면 $W$를 크게 하면 $Loss$가 커지고 반대로 작게하면 $Loss$가 작아지는 걸 알 수 있습니다.
만약 음수라면 $W$를 크게 하면 $Loss$가 작아지고 반대로 작게하면 $Loss$는 작아집니다.
이해하기 쉬운 그래프를 하나 더 올리겠습니다.
[ 그래프 ]
Y축은 Loss이고, X축은 W라고 생각하시면 됩니다.

이제 Gradient가 무엇인지 알 수 있습니다. 위의 그림에서 **우리는 경사를 따라 내려가는 학습을 진행합니다.** 그리고 그 경사가 **Gradient** 입니다.
그리고 이러한 학습 방식이 **Gradient Descent** 입니다.

### 그러면 Gradient Descent는 어떻게 진행할까?

Gradient Descent는 $W$의 순간변화율을 구하고 $Loss$가 최소가 될 수 있는 방향으로 이동하는 방법입니다.

계속해서 $Loss$가 최소가 되는 방향으로 학습을 하다보면 기울기는 점점 0에 가까워집니다. 그러다보면 점점 학습은 수렴하게 되는 것입니다.

여기까지만 계산하면 다음의 식이 나오겠네요.

$W = W - \frac{\partial{Loss}}{\partial{x}}$

그런데 이렇게 학습을 진행해버리면 문제가 생깁니다. 한 번의 학습으로 $W$가 너무 크게 학습되어버리면 기울기가 0으로 수렴하지 않을 수 있습니다. 혹은 아예 튕겨져 나갈수도? 있겠네요.

[그림1]

여기서 Learning Rate가 등장합니다.

$W = W - lr\frac{\partial{Loss}}{\partial{x}}$

Learning Rate가 있으니 학습은 천천히 진행됩니다. 예를들면 Learning Rate가 0.01이라면 다음의 식이 되겠네요.

$W = W - 0.01\frac{\partial{Loss}}{\partial{x}}$

학습의 진행도 다음처럼 천천히 진행되게 됩니다.

[그림2]

### Gradient Descent의 문제점은 무엇일까?


### 다양한 Gradient Descent 방식

#### SGD

SGD는 Stochastic Gradient Descent의 약자입니다.
Stochastic은 "확률적"이라는 의미입니다. ~~어딘가 믿음직스럽지 않은 이름이네요.~~ 대체 어디가 확률적이라는걸까요?

기존의 GD는 전체 데이터를 기반으로 학습을 진행했습니다. 그리고 위에서 본 것처럼 GD의 문제점이 발생합니다. 그러한 문제점을 극복하기 위해서 SGD가 개발되었습니다.

지역 최소지점에 


#### Momentum

#### Adagrad

인공지능 학습의 진행 과정
1. Hyperthesis 계산
2. Loss 계산
3. Backpropergation 수행
4. Loss 값을 최소화 하기 위한 가중치 수정
